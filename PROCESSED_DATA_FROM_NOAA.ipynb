{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODeqDur6hgJFZoNulLsfeD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aa23amd/NOAA-DATASET-CSV/blob/main/PROCESSED_DATA_FROM_NOAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# NOAA API token and parameters (replace with your actual token)\n",
        "API_TOKEN = 'meMJNjoRWdehUATKDiEJKsGcmCjJaUtN'\n",
        "DATASET_ID = 'GHCND'          # Daily Summaries dataset\n",
        "LOCATION_ID = 'FIPS:37'       # Example: North Carolina (adjust as needed)\n",
        "\n",
        "# Define the overall date range (nearly 11 months)\n",
        "overall_start = datetime(2021, 1, 1)\n",
        "overall_end = datetime(2021, 11, 30)\n",
        "\n",
        "# We'll break the overall period into 30-day intervals.\n",
        "INTERVAL_DAYS = 30\n",
        "LIMIT = 1000  # Maximum records per request (pagination limit)\n"
      ],
      "metadata": {
        "id": "jmI3d5qGmk6L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_noaa_data(start_date, end_date, dataset_id, location_id, token, limit=LIMIT):\n",
        "    \"\"\"\n",
        "    Fetch NOAA data for a given date range and handle pagination.\n",
        "    Returns a list of record dictionaries.\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "    offset = 1  # NOAA API offset is 1-indexed\n",
        "    url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data'\n",
        "    headers = {'token': token}\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            'datasetid': dataset_id,\n",
        "            'locationid': location_id,\n",
        "            'startdate': start_date.strftime('%Y-%m-%d'),\n",
        "            'enddate': end_date.strftime('%Y-%m-%d'),\n",
        "            'limit': limit,\n",
        "            'offset': offset\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: {response.status_code} for {start_date.date()} to {end_date.date()} (offset: {offset})\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        results = data.get('results', [])\n",
        "        if not results:\n",
        "            break\n",
        "\n",
        "        all_results.extend(results)\n",
        "\n",
        "        # If fewer records than limit are returned, this interval is done.\n",
        "        if len(results) < limit:\n",
        "            break\n",
        "        else:\n",
        "            offset += limit\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "SuSRX_gDmp1A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "current_start = overall_start\n",
        "\n",
        "while current_start <= overall_end:\n",
        "    current_end = min(current_start + timedelta(days=INTERVAL_DAYS - 1), overall_end)\n",
        "\n",
        "    print(f\"Fetching data from {current_start.date()} to {current_end.date()} ...\")\n",
        "    interval_data = fetch_noaa_data(current_start, current_end, DATASET_ID, LOCATION_ID, API_TOKEN)\n",
        "    print(f\"Records fetched in this interval: {len(interval_data)}\")\n",
        "\n",
        "    all_data.extend(interval_data)\n",
        "    current_start = current_end + timedelta(days=1)\n",
        "\n",
        "print(f\"Total records collected: {len(all_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1_dakvdmuvS",
        "outputId": "cd1f9e5d-cad1-4a92-88f6-4e33fae91db6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from 2021-01-01 to 2021-01-30 ...\n",
            "Error: 503 for 2021-01-01 to 2021-01-30 (offset: 22001)\n",
            "Records fetched in this interval: 22000\n",
            "Fetching data from 2021-01-31 to 2021-03-01 ...\n",
            "Error: 503 for 2021-01-31 to 2021-03-01 (offset: 24001)\n",
            "Records fetched in this interval: 24000\n",
            "Fetching data from 2021-03-02 to 2021-03-31 ...\n",
            "Error: 503 for 2021-03-02 to 2021-03-31 (offset: 7001)\n",
            "Records fetched in this interval: 7000\n",
            "Fetching data from 2021-04-01 to 2021-04-30 ...\n",
            "Error: 502 for 2021-04-01 to 2021-04-30 (offset: 17001)\n",
            "Records fetched in this interval: 17000\n",
            "Fetching data from 2021-05-01 to 2021-05-30 ...\n",
            "Error: 503 for 2021-05-01 to 2021-05-30 (offset: 9001)\n",
            "Records fetched in this interval: 9000\n",
            "Fetching data from 2021-05-31 to 2021-06-29 ...\n",
            "Error: 503 for 2021-05-31 to 2021-06-29 (offset: 20001)\n",
            "Records fetched in this interval: 20000\n",
            "Fetching data from 2021-06-30 to 2021-07-29 ...\n",
            "Error: 503 for 2021-06-30 to 2021-07-29 (offset: 41001)\n",
            "Records fetched in this interval: 41000\n",
            "Fetching data from 2021-07-30 to 2021-08-28 ...\n",
            "Error: 503 for 2021-07-30 to 2021-08-28 (offset: 10001)\n",
            "Records fetched in this interval: 10000\n",
            "Fetching data from 2021-08-29 to 2021-09-27 ...\n",
            "Error: 503 for 2021-08-29 to 2021-09-27 (offset: 2001)\n",
            "Records fetched in this interval: 2000\n",
            "Fetching data from 2021-09-28 to 2021-10-27 ...\n",
            "Error: 503 for 2021-09-28 to 2021-10-27 (offset: 55001)\n",
            "Records fetched in this interval: 55000\n",
            "Fetching data from 2021-10-28 to 2021-11-26 ...\n",
            "Records fetched in this interval: 70963\n",
            "Fetching data from 2021-11-27 to 2021-11-30 ...\n",
            "Records fetched in this interval: 9745\n",
            "Total records collected: 287708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the collected data into a DataFrame\n",
        "raw_df = pd.DataFrame(all_data)\n",
        "print(\"Raw DataFrame shape:\", raw_df.shape)\n",
        "print(raw_df.head())\n",
        "\n",
        "# Save the raw data to a CSV file in the Colab environment\n",
        "raw_csv_filename = 'raw_noaa_data_large.csv'\n",
        "raw_df.to_csv(raw_csv_filename, index=False)\n",
        "print(f\"Raw data saved as '{raw_csv_filename}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcXwORtqxSLy",
        "outputId": "5f70cc55-4eaa-4bbf-a2c1-4138089b6cde"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw DataFrame shape: (287708, 5)\n",
            "                  date datatype            station attributes  value\n",
            "0  2021-01-01T00:00:00     PRCP  GHCND:US1NCAG0005   ,,N,0700     15\n",
            "1  2021-01-01T00:00:00     PRCP  GHCND:US1NCAG0007   ,,N,0700     10\n",
            "2  2021-01-01T00:00:00     PRCP  GHCND:US1NCAL0014   ,,N,0900     13\n",
            "3  2021-01-01T00:00:00     PRCP  GHCND:US1NCAL0036   ,,N,0800      5\n",
            "4  2021-01-01T00:00:00     PRCP  GHCND:US1NCAL0038   ,,N,0700     10\n",
            "Raw data saved as 'raw_noaa_data_large.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(raw_csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "drIQduoGxY2C",
        "outputId": "75495c30-dca3-4e82-a157-1bd75f797202"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_04772e66-c067-49af-b8f7-da428953da62\", \"raw_noaa_data_large.csv\", 16105938)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING RAW DATA"
      ],
      "metadata": {
        "id": "4Sro_kJmzghr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load raw data (adjust the filename if necessary)\n",
        "raw_df = pd.read_csv('raw_noaa_data_large.csv')\n",
        "print(\"Raw DataFrame shape:\", raw_df.shape)\n",
        "print(raw_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgBCaeE2zjyN",
        "outputId": "245f71d8-88b5-4787-f3fe-2396836dd368"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw DataFrame shape: (287708, 5)\n",
            "                  date datatype            station attributes  value\n",
            "0  2021-01-01T00:00:00     PRCP  GHCND:US1NCAG0005   ,,N,0700     15\n",
            "1  2021-01-01T00:00:00     PRCP  GHCND:US1NCAG0007   ,,N,0700     10\n",
            "2  2021-01-01T00:00:00     PRCP  GHCND:US1NCAL0014   ,,N,0900     13\n",
            "3  2021-01-01T00:00:00     PRCP  GHCND:US1NCAL0036   ,,N,0800      5\n",
            "4  2021-01-01T00:00:00     PRCP  GHCND:US1NCAL0038   ,,N,0700     10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PIVOT DATA FRAME"
      ],
      "metadata": {
        "id": "pVTQKQZ7zxmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot the DataFrame:\n",
        "# Each unique date becomes a row and each datatype becomes its own column.\n",
        "df_pivot = raw_df.pivot_table(index='date', columns='datatype', values='value', aggfunc='first').reset_index()\n"
      ],
      "metadata": {
        "id": "Lv8w1hu-z1xt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONVERTING DATE COLUMN TO DATETIME"
      ],
      "metadata": {
        "id": "g94rm3gg0CGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' column to datetime\n",
        "df_pivot['date'] = pd.to_datetime(df_pivot['date'], errors='coerce')\n",
        "print(\"\\nPivoted DataFrame:\")\n",
        "print(df_pivot.head())\n",
        "print(\"Pivoted DataFrame shape:\", df_pivot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPEKLDvE0I4_",
        "outputId": "3eb6c96c-dcdb-47e0-b27b-0b17ae025fb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pivoted DataFrame:\n",
            "datatype       date  ADPT     ASLP    ASTP  AWBT  AWND  DAPR  EVAP   MDPR  \\\n",
            "0        2021-01-01  44.0  10213.0  9793.0  50.0  31.0  11.0   NaN  533.0   \n",
            "1        2021-01-02  61.0  10149.0  9726.0  67.0   0.0   2.0   NaN  239.0   \n",
            "2        2021-01-03  50.0  10125.0  9702.0  67.0   0.0   3.0   NaN  251.0   \n",
            "3        2021-01-04 -11.0  10149.0  9726.0  28.0   0.0   4.0   NaN  287.0   \n",
            "4        2021-01-05  11.0  10132.0  9705.0  33.0  31.0   2.0   NaN    0.0   \n",
            "\n",
            "datatype    PGTM  ...   WSF5   WSFG  WT01  WT02  WT03  WT04  WT05  WT06  WT08  \\\n",
            "0         2358.0  ...  116.0  197.0   1.0   1.0   1.0   NaN   NaN   NaN   1.0   \n",
            "1          258.0  ...   45.0  237.0   1.0   1.0   NaN   NaN   NaN   NaN   NaN   \n",
            "2          648.0  ...   72.0  130.0   1.0   1.0   1.0   NaN   NaN   NaN   NaN   \n",
            "3           16.0  ...   49.0  197.0   1.0   NaN   NaN   NaN   NaN   1.0   NaN   \n",
            "4         1538.0  ...   81.0  116.0   1.0   1.0   NaN   NaN   NaN   NaN   NaN   \n",
            "\n",
            "datatype  WT11  \n",
            "0          1.0  \n",
            "1          NaN  \n",
            "2          NaN  \n",
            "3          NaN  \n",
            "4          NaN  \n",
            "\n",
            "[5 rows x 39 columns]\n",
            "Pivoted DataFrame shape: (129, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA CLEANING"
      ],
      "metadata": {
        "id": "M3VxLxMA0hYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify weather parameter columns (all except 'date')\n",
        "weather_cols = df_pivot.columns.drop('date')\n",
        "\n",
        "# Convert these columns to numeric (errors will be coerced to NaN)\n",
        "for col in weather_cols:\n",
        "    df_pivot[col] = pd.to_numeric(df_pivot[col], errors='coerce')\n",
        "\n",
        "# Fill missing values:\n",
        "# 1. Use forward fill to propagate last valid observation.\n",
        "df_pivot.ffill(inplace=True)\n",
        "# 2. Fill any remaining NaNs with 0 (or use a different strategy if needed).\n",
        "df_pivot.fillna(0, inplace=True)\n",
        "\n",
        "# Sort DataFrame by date (ensures correct time order)\n",
        "df_pivot.sort_values('date', inplace=True)\n",
        "\n",
        "print(\"\\nCleaned DataFrame preview:\")\n",
        "print(df_pivot.head())\n",
        "print(\"DataFrame shape after cleaning:\", df_pivot.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh3rB1wJ0j4p",
        "outputId": "8f0e8c01-cce0-4482-90ed-4aac69960f58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned DataFrame preview:\n",
            "datatype       date  ADPT     ASLP    ASTP  AWBT  AWND  DAPR  EVAP   MDPR  \\\n",
            "0        2021-01-01  44.0  10213.0  9793.0  50.0  31.0  11.0   0.0  533.0   \n",
            "1        2021-01-02  61.0  10149.0  9726.0  67.0   0.0   2.0   0.0  239.0   \n",
            "2        2021-01-03  50.0  10125.0  9702.0  67.0   0.0   3.0   0.0  251.0   \n",
            "3        2021-01-04 -11.0  10149.0  9726.0  28.0   0.0   4.0   0.0  287.0   \n",
            "4        2021-01-05  11.0  10132.0  9705.0  33.0  31.0   2.0   0.0    0.0   \n",
            "\n",
            "datatype    PGTM  ...   WSF5   WSFG  WT01  WT02  WT03  WT04  WT05  WT06  WT08  \\\n",
            "0         2358.0  ...  116.0  197.0   1.0   1.0   1.0   0.0   0.0   0.0   1.0   \n",
            "1          258.0  ...   45.0  237.0   1.0   1.0   1.0   0.0   0.0   0.0   1.0   \n",
            "2          648.0  ...   72.0  130.0   1.0   1.0   1.0   0.0   0.0   0.0   1.0   \n",
            "3           16.0  ...   49.0  197.0   1.0   1.0   1.0   0.0   0.0   1.0   1.0   \n",
            "4         1538.0  ...   81.0  116.0   1.0   1.0   1.0   0.0   0.0   1.0   1.0   \n",
            "\n",
            "datatype  WT11  \n",
            "0          1.0  \n",
            "1          1.0  \n",
            "2          1.0  \n",
            "3          1.0  \n",
            "4          1.0  \n",
            "\n",
            "[5 rows x 39 columns]\n",
            "DataFrame shape after cleaning: (129, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "xRoFugqE02lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the 'PRCP' column exists\n",
        "if 'PRCP' in df_pivot.columns:\n",
        "    # Create lag features: previous day's and two days ago's PRCP\n",
        "    df_pivot['PRCP_lag1'] = df_pivot['PRCP'].shift(1)\n",
        "    df_pivot['PRCP_lag2'] = df_pivot['PRCP'].shift(2)\n",
        "\n",
        "    # Drop rows with NaN values resulting from shifting (this removes the first two rows)\n",
        "    df_pivot.dropna(inplace=True)\n",
        "else:\n",
        "    print(\"PRCP column not found; please check your raw data.\")\n",
        "\n",
        "print(\"\\nDataFrame after creating lag features:\")\n",
        "print(df_pivot.head())\n",
        "print(\"Shape after lag feature creation:\", df_pivot.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg8w567407Qx",
        "outputId": "0edc970b-a037-479d-82c1-425e3f20ac7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after creating lag features:\n",
            "datatype       date  ADPT     ASLP    ASTP  AWBT  AWND  DAPR  EVAP   MDPR  \\\n",
            "2        2021-01-03  50.0  10125.0  9702.0  67.0   0.0   3.0   0.0  251.0   \n",
            "3        2021-01-04 -11.0  10149.0  9726.0  28.0   0.0   4.0   0.0  287.0   \n",
            "4        2021-01-05  11.0  10132.0  9705.0  33.0  31.0   2.0   0.0    0.0   \n",
            "5        2021-01-06 -39.0  10207.0  9776.0  11.0   0.0  12.0  15.0  869.0   \n",
            "6        2021-01-07 -28.0  10200.0  9776.0   0.0   0.0   2.0  10.0   13.0   \n",
            "\n",
            "datatype    PGTM  ...  WT01  WT02  WT03  WT04  WT05  WT06  WT08  WT11  \\\n",
            "2          648.0  ...   1.0   1.0   1.0   0.0   0.0   0.0   1.0   1.0   \n",
            "3           16.0  ...   1.0   1.0   1.0   0.0   0.0   1.0   1.0   1.0   \n",
            "4         1538.0  ...   1.0   1.0   1.0   0.0   0.0   1.0   1.0   1.0   \n",
            "5         1326.0  ...   1.0   1.0   1.0   0.0   0.0   1.0   1.0   1.0   \n",
            "6         2340.0  ...   1.0   1.0   1.0   0.0   0.0   1.0   1.0   1.0   \n",
            "\n",
            "datatype  PRCP_lag1  PRCP_lag2  \n",
            "2             193.0       15.0  \n",
            "3               5.0      193.0  \n",
            "4               0.0        5.0  \n",
            "5               0.0        0.0  \n",
            "6               0.0        0.0  \n",
            "\n",
            "[5 rows x 41 columns]\n",
            "Shape after lag feature creation: (127, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NORMALIZATION"
      ],
      "metadata": {
        "id": "F4_bBtCb1AVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Identify columns to scale (all columns except 'date')\n",
        "cols_to_scale = df_pivot.columns.drop('date')\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_pivot[cols_to_scale] = scaler.fit_transform(df_pivot[cols_to_scale])\n",
        "\n",
        "print(\"\\nScaled DataFrame preview:\")\n",
        "print(df_pivot.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHHwJawj1DQp",
        "outputId": "7ef06111-045d-4b70-91b2-1394a823321c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scaled DataFrame preview:\n",
            "datatype       date      ADPT      ASLP      ASTP      AWBT      AWND  \\\n",
            "2        2021-01-03  0.525074  0.448795  0.424437  0.363985  0.000000   \n",
            "3        2021-01-04  0.345133  0.521084  0.501608  0.214559  0.000000   \n",
            "4        2021-01-05  0.410029  0.469880  0.434084  0.233716  0.169399   \n",
            "5        2021-01-06  0.262537  0.695783  0.662379  0.149425  0.000000   \n",
            "6        2021-01-07  0.294985  0.674699  0.662379  0.107280  0.000000   \n",
            "\n",
            "datatype      DAPR      EVAP      MDPR      PGTM  ...  WT01  WT02  WT03  WT04  \\\n",
            "2         0.018519  0.000000  0.190008  0.271945  ...   0.0   0.0   0.0   0.0   \n",
            "3         0.037037  0.000000  0.217260  0.000000  ...   0.0   0.0   0.0   0.0   \n",
            "4         0.000000  0.000000  0.000000  0.654905  ...   0.0   0.0   0.0   0.0   \n",
            "5         0.185185  0.067873  0.657835  0.563683  ...   0.0   0.0   0.0   0.0   \n",
            "6         0.000000  0.045249  0.009841  1.000000  ...   0.0   0.0   0.0   0.0   \n",
            "\n",
            "datatype  WT05  WT06  WT08  WT11  PRCP_lag1  PRCP_lag2  \n",
            "2          0.0   0.0   0.0   0.0   0.358736   0.027881  \n",
            "3          0.0   1.0   0.0   0.0   0.009294   0.358736  \n",
            "4          0.0   1.0   0.0   0.0   0.000000   0.009294  \n",
            "5          0.0   1.0   0.0   0.0   0.000000   0.000000  \n",
            "6          0.0   1.0   0.0   0.0   0.000000   0.000000  \n",
            "\n",
            "[5 rows x 41 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for XGBoost"
      ],
      "metadata": {
        "id": "1eVplpFM1Q-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target (adjust if you add more features later)\n",
        "features = ['PRCP_lag1', 'PRCP_lag2']\n",
        "target = 'PRCP'\n",
        "\n",
        "# Verify that required columns exist\n",
        "missing_cols = [col for col in features + [target] if col not in df_pivot.columns]\n",
        "if missing_cols:\n",
        "    print(\"Missing columns for tabular model:\", missing_cols)\n",
        "else:\n",
        "    X = df_pivot[features]\n",
        "    y = df_pivot[target]\n",
        "\n",
        "    # Use sequential split for time series data: first 80% for training, last 20% for testing.\n",
        "    train_size = int(len(df_pivot) * 0.8)\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "    print(\"\\nTabular Data Shapes:\")\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"X_test shape:\", X_test.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wArWkn21cJk",
        "outputId": "c4fa56cc-5ed1-455e-fc6d-9de2dbd0712d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tabular Data Shapes:\n",
            "X_train shape: (101, 2)\n",
            "X_test shape: (26, 2)\n",
            "y_train shape: (101,)\n",
            "y_test shape: (26,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Sequence-Based Models LSTM"
      ],
      "metadata": {
        "id": "qxkTxw851mwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEINE FUNCTION"
      ],
      "metadata": {
        "id": "P5yV3kGb14Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(data, seq_length, feature_cols, target_col):\n",
        "    \"\"\"\n",
        "    Create sliding window sequences for sequence models.\n",
        "    Returns:\n",
        "        X_seq: 3D numpy array of shape (samples, seq_length, num_features)\n",
        "        y_seq: 1D numpy array of targets.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data[feature_cols].iloc[i:i+seq_length].values\n",
        "        target_val = data[target_col].iloc[i+seq_length]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target_val)\n",
        "    return np.array(sequences), np.array(targets)\n"
      ],
      "metadata": {
        "id": "1nx-rL-o1pWw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATE SEQUENCE"
      ],
      "metadata": {
        "id": "vup1KS9N17cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence parameters\n",
        "sequence_length = 2  # For small data; typically 5 or more for larger datasets.\n",
        "sequence_features = ['PRCP']  # You can include more features here if desired.\n",
        "target_seq = 'PRCP'\n",
        "\n",
        "# Check that required columns exist\n",
        "missing_seq_cols = [col for col in sequence_features + [target_seq] if col not in df_pivot.columns]\n",
        "if missing_seq_cols:\n",
        "    print(\"Missing columns for sequence model:\", missing_seq_cols)\n",
        "else:\n",
        "    X_seq, y_seq = create_sequences(df_pivot, sequence_length, sequence_features, target_seq)\n",
        "    print(\"\\nSequence Data Shapes:\")\n",
        "    print(\"X_seq shape (samples, timesteps, features):\", X_seq.shape)\n",
        "    print(\"y_seq shape:\", y_seq.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHKiggfs1_ne",
        "outputId": "a7257b74-d085-4993-df73-d3bf31673d53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequence Data Shapes:\n",
            "X_seq shape (samples, timesteps, features): (125, 2, 1)\n",
            "y_seq shape: (125,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA SAVING PROCESSED"
      ],
      "metadata": {
        "id": "Tu4cDSEk2GnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed DataFrame to CSV\n",
        "df_pivot.to_csv('processed_noaa_weather_data.csv', index=False)\n",
        "print(\"\\nProcessed data saved as 'processed_noaa_weather_data.csv'\")\n",
        "\n",
        "# Save sequence data as NumPy arrays\n",
        "np.save('X_seq.npy', X_seq)\n",
        "np.save('y_seq.npy', y_seq)\n",
        "print(\"Sequence data saved as 'X_seq.npy' and 'y_seq.npy'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLA223yQ2LED",
        "outputId": "f6772487-f584-45a7-bf25-6000686e4379"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed data saved as 'processed_noaa_weather_data.csv'\n",
            "Sequence data saved as 'X_seq.npy' and 'y_seq.npy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Replace the filename if necessary\n",
        "files.download('processed_noaa_weather_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "71rs9xId3Uxl",
        "outputId": "fd52f158-81fe-4915-c017-1b91fde00e04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9c384d47-37c6-4d02-8379-0972914876d2\", \"processed_noaa_weather_data.csv\", 68641)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}